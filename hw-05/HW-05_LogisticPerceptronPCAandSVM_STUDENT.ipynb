{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment #5 (Individual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"text-align: right;\"> &#9989; Rajmeet Singh Chandok</p>\n",
    "### <p style=\"text-align: right;\"> &#9989; rajmeet</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals for this homework assignment\n",
    "\n",
    "By the end of this assignment, you should be able to:\n",
    "* Use `git` to track your work and turn in your assignment\n",
    "* Read in data and prepare it for modeling\n",
    "* Build, fit, and evaluate Logistic Regression models\n",
    "* Build, fit, and evaluate Perceptron models\n",
    "* Use PCA to reduce the number of features\n",
    "* Build, fit, and evaluate an SVC model on PCA-transformed data\n",
    "* Systematically investigate the effects of the number of PCA components on an SVC model of data\n",
    "\n",
    "### Assignment instructions:\n",
    "\n",
    "Work through the following assignment, making sure to follow all of the directions and answer all of the questions.\n",
    "\n",
    "There are **41 points** possible on this assignment. Point values for each part are included in the section headers.\n",
    "\n",
    "This assignment is **due at 11:59 pm on Friday, April 15th**. It should be uploaded into the \"Homework Assignments\" submission folder for Homework #5. Submission instructions can be found at the end of the notebook.. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Add to your Git repository to track your progress on your assignment (4 points)\n",
    "\n",
    "For this assignment, you're going to add it to the `cmse202-s22-turnin` repository you created in class so that you can track your progress on the assignment and preserve the final version that you turn in. In order to do this you need to\n",
    "\n",
    "**&#9989; Do the following**:\n",
    "\n",
    "1. Navigate to your `cmse202-s22-turnin` repository and create a new directory called `hw-05`.\n",
    "2. Move this notebook into that **new directory** in your repository, then **add it and commit it to your repository**.\n",
    "1. Finally, to test that everything is working, \"git push\" the file so that it ends up in your GitHub repository.\n",
    "\n",
    "**Important**: Double check you've added your Professor and your TA as collaborators to your \"turnin\" repository (you should have done this in the previous homework assignment).\n",
    "\n",
    "**Also important**: Make sure that the version of this notebook that you are working on is the same one that you just added to your repository! If you are working on a different copy of the notebook, **none of your changes will be tracked**!\n",
    "\n",
    "If everything went as intended, the file should now show up on your GitHub account in the \"`cmse202-s22-turnin`\" repository inside the `hw-05` directory that you just created.  Periodically, **you'll be asked to commit your changes to the repository and push them to the remote GitHub location**. Of course, you can always commit your changes more often than that, if you wish.  It can be good to get into a habit of committing your changes any time you make a significant modification, or when you stop working on the project for a bit.\n",
    "\n",
    "&#9989; **Do this**: Before you move on, put the command that your instructor should run to clone your repository in the markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "git clone https://github.com/Rajmeet/cmse202-s22-turnin.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Do this**: Before you move on, create a new branch called `hw05_branch` and move into it. In the cell below put the command(s) to create a new branch and to checkout the new branch. (_Note_: your TA will be able to see if you have created the branch and its history)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "git checkout -b hw05_branch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Do this**: Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"loading\"></a>\n",
    "## Part 2. Logistic Regression (12 points)\n",
    "### 2.1 Data processing (5 points)\n",
    "For this part, you will read and process the dataset `hw5_data.csv` and split the training and testing sets.\n",
    "\n",
    "The provided data corresponds to a molecular biology dataset, where each row represents a patient classified into either \"active\" or \"repressive\". The columns represent features, where each feature comes from the quantification of a specific gene. Ten genes (ten features) are measured. The goal is to make predictive models that can classify patients (\"active\" or \"repressive\") based on the ten features.\n",
    "\n",
    "The dataset is located at:\n",
    "`https://raw.githubusercontent.com/msu-cmse-courses/cmse202-S22-data/main/data/hw5_data.csv`\n",
    "\n",
    "\n",
    "**&#9989; Question 2.1.1 (1 point):** Read the `hw5_data.csv` file into your notebook and print out the unique labels in the `label` columns. \n",
    "\n",
    "Note: each row represents one data point and each column (except the `label` column) represents one feature. The `label` column corresponds to the class labels for every data point. There are two types of unique class labels in the `label` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['active', 'repressive'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('hw5_data.csv')\n",
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 2.1.2 (1 point):** To simplify the process of data modeling, we should convert the labels from strings to integers.\n",
    "\n",
    "Replace all of the strings in your `label` column with integers based on the following:\n",
    "\n",
    "| original label | integer label |\n",
    "| -------- | -------- |\n",
    "| repressive | 0 |\n",
    "| active | 1 |\n",
    "\n",
    "Once you've replaced the labels, display your DataFrame and confirm that it looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>1.089799</td>\n",
       "      <td>3.706677</td>\n",
       "      <td>1.839418</td>\n",
       "      <td>1.000414</td>\n",
       "      <td>0.751246</td>\n",
       "      <td>-0.077189</td>\n",
       "      <td>0.949589</td>\n",
       "      <td>1.641961</td>\n",
       "      <td>1.102132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.355099</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.247742</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194369</td>\n",
       "      <td>0.116891</td>\n",
       "      <td>-0.059497</td>\n",
       "      <td>1.086607</td>\n",
       "      <td>0.508670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.814211</td>\n",
       "      <td>2.696122</td>\n",
       "      <td>0.221476</td>\n",
       "      <td>0.229138</td>\n",
       "      <td>-0.173686</td>\n",
       "      <td>1.091221</td>\n",
       "      <td>1.048915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.690140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.687331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.527291</td>\n",
       "      <td>0.185705</td>\n",
       "      <td>-0.089479</td>\n",
       "      <td>-0.379929</td>\n",
       "      <td>-0.093369</td>\n",
       "      <td>0.272125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.376770</td>\n",
       "      <td>0.631267</td>\n",
       "      <td>2.090756</td>\n",
       "      <td>1.581667</td>\n",
       "      <td>0.793976</td>\n",
       "      <td>0.846570</td>\n",
       "      <td>0.178551</td>\n",
       "      <td>0.245401</td>\n",
       "      <td>1.221811</td>\n",
       "      <td>0.111456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0</td>\n",
       "      <td>1.539002</td>\n",
       "      <td>0.499277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.934047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750042</td>\n",
       "      <td>0.605361</td>\n",
       "      <td>0.436925</td>\n",
       "      <td>1.426063</td>\n",
       "      <td>0.540208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0</td>\n",
       "      <td>0.004665</td>\n",
       "      <td>0.356522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.646765</td>\n",
       "      <td>0.198733</td>\n",
       "      <td>0.605987</td>\n",
       "      <td>0.306014</td>\n",
       "      <td>0.437594</td>\n",
       "      <td>1.220802</td>\n",
       "      <td>0.908293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0</td>\n",
       "      <td>0.368634</td>\n",
       "      <td>2.356287</td>\n",
       "      <td>1.937731</td>\n",
       "      <td>0.684271</td>\n",
       "      <td>1.801650</td>\n",
       "      <td>1.492007</td>\n",
       "      <td>0.463434</td>\n",
       "      <td>1.313578</td>\n",
       "      <td>0.908521</td>\n",
       "      <td>0.958253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.668051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.213437</td>\n",
       "      <td>0.403462</td>\n",
       "      <td>1.526797</td>\n",
       "      <td>0.274236</td>\n",
       "      <td>1.283074</td>\n",
       "      <td>0.795663</td>\n",
       "      <td>0.757853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0</td>\n",
       "      <td>1.177402</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.970539</td>\n",
       "      <td>0.119612</td>\n",
       "      <td>0.777841</td>\n",
       "      <td>-0.106966</td>\n",
       "      <td>0.368507</td>\n",
       "      <td>0.138330</td>\n",
       "      <td>0.340818</td>\n",
       "      <td>-0.378622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0        1   0.000168   1.089799   3.706677   1.839418   1.000414   0.751246   \n",
       "1        1   0.355099   0.000000   2.247742        NaN   0.000000   0.194369   \n",
       "2        1   0.001236   0.000000        NaN   0.814211   2.696122   0.221476   \n",
       "3        1   0.690140   0.000000   0.687331   0.000000   0.527291   0.185705   \n",
       "4        1   1.376770   0.631267   2.090756   1.581667   0.793976   0.846570   \n",
       "..     ...        ...        ...        ...        ...        ...        ...   \n",
       "995      0   1.539002   0.499277   0.000000   1.934047   0.000000   0.750042   \n",
       "996      0   0.004665   0.356522   0.000000   1.646765   0.198733   0.605987   \n",
       "997      0   0.368634   2.356287   1.937731   0.684271   1.801650   1.492007   \n",
       "998      0   0.000000   1.668051   0.000000   1.213437   0.403462   1.526797   \n",
       "999      0   1.177402   0.000000   1.970539   0.119612   0.777841  -0.106966   \n",
       "\n",
       "     feature_7  feature_8  feature_9  feature_10  \n",
       "0    -0.077189   0.949589   1.641961    1.102132  \n",
       "1     0.116891  -0.059497   1.086607    0.508670  \n",
       "2     0.229138  -0.173686   1.091221    1.048915  \n",
       "3    -0.089479  -0.379929  -0.093369    0.272125  \n",
       "4     0.178551   0.245401   1.221811    0.111456  \n",
       "..         ...        ...        ...         ...  \n",
       "995   0.605361   0.436925   1.426063    0.540208  \n",
       "996   0.306014   0.437594   1.220802    0.908293  \n",
       "997   0.463434   1.313578   0.908521    0.958253  \n",
       "998   0.274236   1.283074   0.795663    0.757853  \n",
       "999   0.368507   0.138330   0.340818   -0.378622  \n",
       "\n",
       "[1000 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = df['label'].replace(['repressive', 'active'], [0, 1])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 2.1.3 (1 point):** At this point, you've hopefully noticed that some of the rows seems to have missing data values as indicated by the existence of `NaN` values. Since we don't necessarily know what to replace these values with, let's just play it safe and remove all of the rows that have `NaN` in any of the column entries. This should help to ensure that we don't end up with errors or confusing results when we try to classify the data.\n",
    "\n",
    "Remove all of the rows that contain a `NaN` in any column. **Make sure you actually store this new version of your dataframe either in the original variable name or in a new variable name**. If everything went as intended, you should find that you have 793 rows left over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(793, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 2.1.4 (1 point):** As we've seen when working with `sklearn` it can be much easier to work with the data if we have separate variables: one that stores the feature matrix and one that stores the class labels.\n",
    "\n",
    "Split your DataFrame so that you have two separate DataFrames: (1) one called `features`, which contains all columns of features; and (2) one called `labels`, which is a single-column dataframe that contains all of the *new* integer labels you just created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop('label', axis=1)\n",
    "labels = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 2.1.5 (1 point):** How balanced is your dataset? You need to write a bit of code to figure out how balanced your dataset is, by counting the numbers of data points of each classe label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    402\n",
       "1    391\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> We can use value_counts() to count the number of data points for each class labels. In this case $0$ (repressive) has a count of $402$ and $1$ (active) has a count of $391$. This is about 49.5 to 50.5 ratio or approxiately a 50:50 ratio which implies that the dataset is balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.2 Logistic Regression (7 points)\n",
    "\n",
    "For this part, you will apply logistic regression to tackle th classification problem: predicting class labels based on the features.\n",
    "\n",
    "**&#9989; Question 2.2.1 (1 point):** Split your data into a training and a testing set with a training set representing 75% of your data. For reproducibility , set the `random_state` argument to `314159`. Print the lengths to show you have the right number of entries for the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((594, 10), (199, 10))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectors, test_vectors, train_labels, test_labels = train_test_split(features, labels, test_size=0.25, random_state=314159)\n",
    "train_vectors.shape, test_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 2.2.2 (3 points):** Build a Logistic regression model based on default settings.\n",
    "\n",
    "Add constant term in both training and testing features, fit Logistic regression based on the training set, and then print out the model summary.\n",
    "\n",
    "**Note:** You can use the built-in model `Logit` in `statsmodels.api`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.620529\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  label   No. Observations:                  594\n",
      "Model:                          Logit   Df Residuals:                      583\n",
      "Method:                           MLE   Df Model:                           10\n",
      "Date:                Mon, 18 Apr 2022   Pseudo R-squ.:                  0.1039\n",
      "Time:                        12:18:01   Log-Likelihood:                -368.59\n",
      "converged:                       True   LL-Null:                       -411.32\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.244e-14\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.8952      0.234     -3.830      0.000      -1.353      -0.437\n",
      "feature_1      0.5892      0.158      3.721      0.000       0.279       0.900\n",
      "feature_2      0.2239      0.427      0.525      0.600      -0.612       1.060\n",
      "feature_3      0.3242      0.072      4.475      0.000       0.182       0.466\n",
      "feature_4     -0.3273      0.332     -0.985      0.325      -0.979       0.324\n",
      "feature_5     -0.0688      0.150     -0.460      0.646      -0.362       0.224\n",
      "feature_6      0.2728      0.462      0.590      0.555      -0.633       1.179\n",
      "feature_7     -0.3599      0.437     -0.823      0.410      -1.217       0.497\n",
      "feature_8     -0.2714      0.440     -0.617      0.537      -1.133       0.590\n",
      "feature_9     -0.0728      0.435     -0.167      0.867      -0.925       0.780\n",
      "feature_10    -0.0810      0.255     -0.317      0.751      -0.581       0.419\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "logit_model = sm.Logit(train_labels, sm.add_constant(train_vectors))\n",
    "result = logit_model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 2.2.3 (1 point):** What is the Pseudo R^2? Which features have p-value < 0.05?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> The pseudo $R^2$ value is 0.1039. Moreover, feature_1 and feature_3 have p_value < 0.05 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 2.2.4 (2 points):** Make predictions for the testing set using the trained model.\n",
    "\n",
    "Note: the logistic regression model predicts the probability of belonging to class 1. To make the final binary classification, let's the threshold to be 0.5, which means that every sample in the testing set with predicted probability scores greater than 0.5 will be predicted as '1', and other samples with predicted probability less than 0.5 will be predicted as '0'. \n",
    "\n",
    "Show the model's accuracy score based on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6683417085427136"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = result.predict(sm.add_constant(test_vectors))\n",
    "predictions[predictions > 0.5] = 1\n",
    "predictions[predictions <= 0.5] = 0\n",
    "accuracy_score(predictions, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository using the commit message \"Committing Part 2\", and push the changes to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3. Perceptron (5 points)\n",
    "\n",
    "For this part, you will use another model, Perceptron, to continue working on the same classification problem.\n",
    "\n",
    "**&#9989; Question 3.1 (2 points):** (1) Build a Perceptron model with default settings, and fit the model based on the training set.\n",
    "\n",
    "(2) Apply the trained model on the test features to predict the labels for the testing dataset. \n",
    "\n",
    "(3) Evaluate the model by printing out the confusion matrix and classification report, based on its performance on the testing dataset.\n",
    "\n",
    "**Note:** You can use the built-in model `Perceptron` in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.457286432160804"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Perceptron()\n",
    "p.fit(train_vectors, train_labels)\n",
    "p.score(test_vectors, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 90   4]\n",
      " [104   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.96      0.63        94\n",
      "           1       0.20      0.01      0.02       105\n",
      "\n",
      "    accuracy                           0.46       199\n",
      "   macro avg       0.33      0.48      0.32       199\n",
      "weighted avg       0.32      0.46      0.30       199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(test_labels, p.predict(test_vectors)))\n",
    "print(classification_report(test_labels, p.predict(test_vectors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 3.2 (3 points):**. Finding the best penalty term.\n",
    "\n",
    "`Perceptron` from the `sklearn` can employ different penalty terms, including `l1`, `l2`, and `elasticnet` (Note: check the `penalty` argument of `Perceptron`). Apply the Perceptron on the training dataset again, based on different penalty terms (i.e. make 3 Perceptron models). Print out the accuray score of each model, based on the testing dataset. \n",
    "\n",
    "Which penalty term results in the best accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5527638190954773"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl1 = Perceptron(penalty=\"l1\")\n",
    "pl1.fit(train_vectors.values, train_labels.values)\n",
    "pl1.score(test_vectors, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4723618090452261"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl2 = Perceptron(penalty=\"l2\")\n",
    "pl2.fit(train_vectors.values, train_labels.values)\n",
    "pl2.score(test_vectors, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4723618090452261"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pen = Perceptron(penalty=\"elasticnet\")\n",
    "pen.fit(train_vectors.values, train_labels.values)\n",
    "pen.score(test_vectors, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the penality **l1** we get the highest accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository using the commit message \"Committing Part 3\", and push the changes to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4. Principal Component Analysis (6 points)\n",
    "\n",
    "The full model uses all 10 features to predict the results. In many cases, we might need to see how close we can get with fewer features. But instead of simply removing features, we will use a Principal Component Analysis (PCA) to determine the combined features that contribute the most the model (through their accounted variance).\n",
    "\n",
    "**&#9989; Question 4.1 (1 point):** Do a little bit of data preparation before we perform our PCA.\n",
    "\n",
    "Because the features in our dataset have very different ranges of values, the variation captured by the PCA will be skewed by these relative differences. As a result, it is good practice to **normalize** the features so that they have comparable ranges of values. Thankfully, `sklearn` has a useful function for doing this!\n",
    "\n",
    "```from sklearn.preprocessing import MinMaxScaler```\n",
    "\n",
    "Perform a \"Min-Max\" scaling to normalize the features and store the new normalized features in a new dataframe called as `features_norm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.292383</td>\n",
       "      <td>0.538579</td>\n",
       "      <td>0.504567</td>\n",
       "      <td>0.279789</td>\n",
       "      <td>0.373396</td>\n",
       "      <td>0.084569</td>\n",
       "      <td>0.525016</td>\n",
       "      <td>0.666995</td>\n",
       "      <td>0.682334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.051232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147469</td>\n",
       "      <td>0.219089</td>\n",
       "      <td>0.081816</td>\n",
       "      <td>0.028220</td>\n",
       "      <td>0.136492</td>\n",
       "      <td>0.377400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.102204</td>\n",
       "      <td>0.169363</td>\n",
       "      <td>0.303786</td>\n",
       "      <td>0.433864</td>\n",
       "      <td>0.222054</td>\n",
       "      <td>0.399405</td>\n",
       "      <td>0.141857</td>\n",
       "      <td>0.261885</td>\n",
       "      <td>0.538552</td>\n",
       "      <td>0.318372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.030227</td>\n",
       "      <td>0.158317</td>\n",
       "      <td>0.009161</td>\n",
       "      <td>0.153590</td>\n",
       "      <td>0.103743</td>\n",
       "      <td>0.191606</td>\n",
       "      <td>0.210754</td>\n",
       "      <td>0.247571</td>\n",
       "      <td>0.367096</td>\n",
       "      <td>0.397211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.091911</td>\n",
       "      <td>0.340339</td>\n",
       "      <td>0.220272</td>\n",
       "      <td>0.322154</td>\n",
       "      <td>0.145596</td>\n",
       "      <td>0.502093</td>\n",
       "      <td>0.161927</td>\n",
       "      <td>0.537776</td>\n",
       "      <td>0.456159</td>\n",
       "      <td>0.446565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.114248</td>\n",
       "      <td>0.133951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.530525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.373067</td>\n",
       "      <td>0.237464</td>\n",
       "      <td>0.333451</td>\n",
       "      <td>0.600994</td>\n",
       "      <td>0.475890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.095651</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.451721</td>\n",
       "      <td>0.055580</td>\n",
       "      <td>0.333762</td>\n",
       "      <td>0.170409</td>\n",
       "      <td>0.333701</td>\n",
       "      <td>0.538244</td>\n",
       "      <td>0.611120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.027365</td>\n",
       "      <td>0.632170</td>\n",
       "      <td>0.281552</td>\n",
       "      <td>0.187701</td>\n",
       "      <td>0.503873</td>\n",
       "      <td>0.575510</td>\n",
       "      <td>0.205672</td>\n",
       "      <td>0.661026</td>\n",
       "      <td>0.442777</td>\n",
       "      <td>0.629475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.332856</td>\n",
       "      <td>0.112837</td>\n",
       "      <td>0.585003</td>\n",
       "      <td>0.163290</td>\n",
       "      <td>0.649628</td>\n",
       "      <td>0.408276</td>\n",
       "      <td>0.555850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.087404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.286319</td>\n",
       "      <td>0.032811</td>\n",
       "      <td>0.217541</td>\n",
       "      <td>0.139235</td>\n",
       "      <td>0.184408</td>\n",
       "      <td>0.221876</td>\n",
       "      <td>0.269226</td>\n",
       "      <td>0.138324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>793 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0     0.000012   0.292383   0.538579   0.504567   0.279789   0.373396   \n",
       "3     0.051232   0.000000   0.099869   0.000000   0.147469   0.219089   \n",
       "4     0.102204   0.169363   0.303786   0.433864   0.222054   0.399405   \n",
       "6     0.030227   0.158317   0.009161   0.153590   0.103743   0.191606   \n",
       "8     0.091911   0.340339   0.220272   0.322154   0.145596   0.502093   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "995   0.114248   0.133951   0.000000   0.530525   0.000000   0.373067   \n",
       "996   0.000346   0.095651   0.000000   0.451721   0.055580   0.333762   \n",
       "997   0.027365   0.632170   0.281552   0.187701   0.503873   0.575510   \n",
       "998   0.000000   0.447522   0.000000   0.332856   0.112837   0.585003   \n",
       "999   0.087404   0.000000   0.286319   0.032811   0.217541   0.139235   \n",
       "\n",
       "     feature_7  feature_8  feature_9  feature_10  \n",
       "0     0.084569   0.525016   0.666995    0.682334  \n",
       "3     0.081816   0.028220   0.136492    0.377400  \n",
       "4     0.141857   0.261885   0.538552    0.318372  \n",
       "6     0.210754   0.247571   0.367096    0.397211  \n",
       "8     0.161927   0.537776   0.456159    0.446565  \n",
       "..         ...        ...        ...         ...  \n",
       "995   0.237464   0.333451   0.600994    0.475890  \n",
       "996   0.170409   0.333701   0.538244    0.611120  \n",
       "997   0.205672   0.661026   0.442777    0.629475  \n",
       "998   0.163290   0.649628   0.408276    0.555850  \n",
       "999   0.184408   0.221876   0.269226    0.138324  \n",
       "\n",
       "[793 rows x 10 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scalar = MinMaxScaler()\n",
    "\n",
    "features_norm = scalar.fit_transform(features)\n",
    "features_norm = pd.DataFrame(features_norm, columns=features.columns, index=features.index)\n",
    "features_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 4.2 (1 point):** As you did in Question 2.2.1 above, split your new normalized features and corresponding labels (the labels are the same as before) into a training and a testing set, with the training set representing 75% of your data. For reproducibility , set the `random_state` argument to `314159`. Print the lengths to show you have the right number of entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((594, 10), (199, 10))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectors, test_vectors, train_labels, test_labels = train_test_split(features_norm, labels, test_size=0.25, random_state=314159)\n",
    "train_vectors.shape, test_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 4.3 (3 points):** Run a Principle Component Analysis (PCA)\n",
    "\n",
    "Since we only have 10 features to start with, let's see how well we can do if we try to aggressively reduce the feature count and use only **3** principle components. We'll see how well we can predict the labels of dataset with just three!\n",
    "\n",
    "\n",
    "(1) Using `PCA()` and the associated `fit()` method, run a principle component analysis on your training features using only 3 components. \n",
    "\n",
    "(2) Transform both the test and training features using the result of your PCA. \n",
    "\n",
    "(3) Print the `explained_variance_ratio_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.29811269, 0.24310656, 0.17698509])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_components = 3\n",
    "pca = PCA(n_components=n_components, whiten=True)\n",
    "_ = pca.fit(train_vectors)\n",
    "pca_train_vectors = pca.transform(train_vectors)\n",
    "pca_test_vectors = pca.transform(test_vectors)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 4.4 (1 point):** What is the total explained variance ratio captured by the 3 principle components? (just quote the number) How well do you think a model with these many features will perform? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a total variance ratio of 0.7182043490884231. The explained variance ratio is the percentage of variance that is attributed by each of the selected components. If the total value is greater than .80 i.e there are more features, the model will overfit, while if the value is less than 0.6 it underfits. Right now, with 3 features we have a good estimate and it will perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository using the commit message \"Committing Part 4\", and push the changes to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Support vector machine based on PCA (14 points)\n",
    "\n",
    "### 5.1 Support vector machine (6 points)\n",
    "\n",
    "For this part, you will build SVC model using the 3 components from PCA, and do grid search to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 5.1.1 (2 points):** Build a linear SVC model with `C=0.1`, and fit it to the training set (using the 3 PCA components from the training set).\n",
    "\n",
    "Then use the test features to predict the labels for the testing set. \n",
    "\n",
    "Evaluate the model's performance using the **confusion matrix** and **classification report**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.72      0.64        94\n",
      "           1       0.68      0.52      0.59       105\n",
      "\n",
      "    accuracy                           0.62       199\n",
      "   macro avg       0.63      0.62      0.62       199\n",
      "weighted avg       0.63      0.62      0.62       199\n",
      "\n",
      "[[68 26]\n",
      " [50 55]]\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(C=0.1)\n",
    "svc.fit(pca_train_vectors, train_labels)\n",
    "print(classification_report(test_labels, svc.predict(pca_test_vectors)))\n",
    "print(confusion_matrix(test_labels, svc.predict(pca_test_vectors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 5.1.2 (3 points):** Find the best hyperparameters\n",
    "\n",
    "At this point, we have fit one SVC model and determined it's performance, but is it the best model? We can use `GridSearchCV` to find the best model (given our choices of parameters). Once we do that, we will use that \"best\" model for making predictions.\n",
    "\n",
    "Using the following parameters (`C` = `1e-3`, `0.01`, `0.1`, `1`, `10`, `100` and `gamma` = `1e-6`, `1e-5`, `1e-4`, `1e-3`, `0.01`, `0.1`) for both a `linear` and `rbf` kernel use `GridSearchCV` with the `SVC()` model to find the best fit parameters. Once, you've run the grid search, print the \"best estimators\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier to the training set\n",
      "Best estimator found by grid search:\n",
      "SVC(C=0.1, class_weight='balanced', gamma=1e-06, kernel='linear')\n",
      "Best parameters found by grid search:\n",
      "{'C': 0.1, 'gamma': 1e-06, 'kernel': 'linear'}\n",
      "Runtime 5.9650208950042725\n"
     ]
    }
   ],
   "source": [
    "# Train a SVM classification model\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "#make some temporary variables so you can change this easily\n",
    "tmp_vectors = pca_train_vectors\n",
    "tmp_labels = train_labels\n",
    "\n",
    "print(\"Fitting the classifier to the training set\")\n",
    "# a dictionary of hyperparameters: key is the name of the parameter, value is a list of values to test\n",
    "param_grid = {'C': [1e-3, 0.01, 0.1, 1, 10, 100],\n",
    "              'gamma': [1e-6, 1e-5, 1e-4, 1e-3, 0.01, 0.1],\n",
    "              'kernel': ['linear','rbf']}\n",
    "# make a classifier by searching over a classifier and the parameter grid\n",
    "clf = GridSearchCV(SVC(class_weight='balanced'), param_grid)\n",
    "\n",
    "# we have a \"good\" classifier (according to GridSearchCV), how's it look\n",
    "clf = clf.fit(tmp_vectors, tmp_labels)\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(clf.best_estimator_)\n",
    "print(\"Best parameters found by grid search:\")\n",
    "print(clf.best_params_)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Runtime\",end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 5.1.3 (1 point):**  Evaluate the best fit model\n",
    "\n",
    "Now that we have found the \"best estimators\", let's determine how good the fit is.\n",
    "\n",
    "Use the test features to predict the labels, based on the best model. Evaluate the performance using the **confusion matrix** and **classification report**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting names on the test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.60      0.60        94\n",
      "           1       0.64      0.64      0.64       105\n",
      "\n",
      "    accuracy                           0.62       199\n",
      "   macro avg       0.62      0.62      0.62       199\n",
      "weighted avg       0.62      0.62      0.62       199\n",
      "\n",
      "[[56 38]\n",
      " [38 67]]\n"
     ]
    }
   ],
   "source": [
    "predict_vectors = pca_test_vectors\n",
    "true_labels = test_labels\n",
    "\n",
    "print(\"Predicting names on the test set\")\n",
    "pred_labels = clf.predict(predict_vectors)\n",
    "\n",
    "print(classification_report(true_labels, pred_labels))\n",
    "print(confusion_matrix(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 How well does PCA work? (8 points)\n",
    "The number of components we use in our PCA matters. Let's investigate how they matter by systematically building a model for any number of selected components. While this might seem a bit unnecessary for such a simple dataset, **this can be very useful for more complex datasets and models!**\n",
    "\n",
    "**&#9989; Question 5.2.1 (3 points):**\n",
    "\n",
    "To systematically explore how well PCA improves our classification model, we will do this by writing a function that \n",
    "* creates the PCA\n",
    "* creates the SVC model\n",
    "* uses `GridSearchCV` to find the best hyperparameters\n",
    "* predicts the labels using test data\n",
    "* returns the accuracy scores and the explained variance ratio.\n",
    "\n",
    "Just as you did in Question 5.1.2, use the following parameters (`C` = `1e-3`, `0.01`, `0.1`, `1`, `10`, `100` and `gamma` = `1e-6`, `1e-5`, `1e-4`, `1e-3`, `0.01`, `0.1`) for both a `linear` and `rbf` kernel use `GridSearchCV` with the `SVC()` model to find the best fit parameters.\n",
    "\n",
    "So, Your function will take as input:\n",
    "* the number of requested PCA components\n",
    "* the training feature data\n",
    "* the testing feature data\n",
    "* the training data labels\n",
    "* the test data labels\n",
    "\n",
    "and it should **return** the accuracy score for an SVC model fit to pca transformed features and the **total** explained variance ratio (i.e. the sum of the explained variance for each component)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code here\n",
    "\n",
    "def reduced_SVM(n_components, train_features, train_labels, test_features, test_labels):\n",
    "    \n",
    "    pca = PCA(n_components=n_components, whiten=True)\n",
    "    _ = pca.fit(train_features)\n",
    "    pca_train_vectors = pca.transform(train_features)\n",
    "    pca_test_vectors = pca.transform(test_features)\n",
    "    \n",
    "    svc = SVC()\n",
    "    param_grid = {'C': [1e-3, 0.01, 0.1, 1, 10, 100],\n",
    "              'gamma': [1e-6, 1e-5, 1e-4, 1e-3, 0.01, 0.1],\n",
    "              'kernel': ['linear','rbf']}\n",
    "    \n",
    "    clf = GridSearchCV(svc, param_grid)\n",
    "    clf.fit(pca_train_vectors, train_labels)\n",
    "    test_predictions = clf.predict(pca_test_vectors)\n",
    "    return accuracy_score(test_labels, test_predictions), sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 5.2.2 (2 points):**\n",
    "\n",
    "Now that you have created a function that returns the accuracy for a given number of components, we will use that to plot the how the accuracy of your SVC model changes when we increase the number of components used in the PCA.\n",
    "\n",
    "For 1 through 10 components, use your function above to compute and store (as a list) the accuracy of your models and the total explained variance ratio of your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "explained_variance = []\n",
    "for i in range(1, 11):\n",
    "    acc, var = reduced_SVM(i, train_vectors, train_labels, test_vectors, test_labels)\n",
    "    accuracy.append(acc)\n",
    "    explained_variance.append(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 5.2.3 (1 point):** Plot the accuracy vs # of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "comp = [i for i in range(1,11)]\n",
    "plt.xlabel(\"Components\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs Components\")\n",
    "plt.plot(comp, accuracy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 5.2.4 (1 point):** Where does it seem like we have diminishing returns? That is, at what point is there no major increase in accuracy (or perhaps the accuracy is decreased) as we add additional components to the PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> The diminishing returns after 5. In the graph it can be seen that there is no major increase in the accuracy after 5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Task 5.2.5 (1 point):** Plot the total explained variance ratio vs # of components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "comp = [i for i in range(1,11)]\n",
    "plt.xlabel(\"Components\")\n",
    "plt.ylabel(\"Explained Variance Ration\")\n",
    "plt.title(\"Variance Ratio vs Components\")\n",
    "plt.plot(comp, explained_variance)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository using the commit message \"Committing Part 5\", and push the changes to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Assignment wrap-up¶\n",
    "Please fill out the form that appears when you run the code below. **You must completely fill this out in order to receive credit for the assignment!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\n",
    "\"\"\"\n",
    "<iframe \n",
    "\tsrc=\"https://forms.office.com/Pages/ResponsePage.aspx?id=MHEXIi9k2UGSEXQjetVofa-byNJHa0xBs0jOGcRl02lURU83U0ZHUUpWUUFRUzhCQ0JZWDQxVVRUVi4u\" \n",
    "\twidth=\"800px\" \n",
    "\theight=\"600px\" \n",
    "\tframeborder=\"0\" \n",
    "\tmarginheight=\"0\" \n",
    "\tmarginwidth=\"0\">\n",
    "\tLoading...\n",
    "</iframe>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations, you're done!\n",
    "Submit this assignment by uploading it to the course Desire2Learn web page. Go to the \"Homework Assignments\" folder, find the submission folder for Homework #5, and upload your notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
